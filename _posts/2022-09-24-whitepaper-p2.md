---
title: "White-paper: artificial neural networks and percolation (Part 2)"
layout: post
categories: Quantum, Crypto
comments: true
image: /assets/article_images/2022-09-24-whitepaper-p2/whitepaper-cover.jpg
---

In the previous [article](https://manuneuro.github.io/EmmanuelCalvet//quantum,/crypto/2022/09/01/whitepaper-p1.html) we introduced the concept of ellipitic curve, and how it allow for producing a *public ke*y form a *private key*. And I proposed to explore Artificial Neural Network (ANN) as a replacement. Now this is not necessary the most secure option, but this is more an excuse to be using ANN in a very non-mainstream way, so let's dive in!

I formalized the problem of generating a public key as finding a deterministic function $F$, such that:

$$public\_key = F(private\_key)$$

With $F$ being not invertible, meaning that $F^{-1}$ do no exist:

$$F^{-1}(public\_key) \ne private\_key$$

Unfortunately, invertibility in a neural network is not necessarily easy to obtain. This is because you can train a neural network to map $y=f(x)$, but you can also in theory train another neural net to get an approximation of the inverse $x=f'(x)$. However, rembember that in the first part, we gave ourselves the challenge of not using learning

This way we are going to dive into a deeper understanding of the properties needed to find a solution to the problem of desining $F$!

***

## Designing the neural network

We are going to use a binary activation function, so I'm going to use the image form the previous article, as it is going to fix ideas on what we are doing:

![The artificial neuron is a mathematical simplification of a biological neuron, composed of an activation function f (here, we use the step function). The neuron receives a weighted (W) sum of inputs (X), such that its output is y=f(X, W). Note that the action of the neuron is completely deterministic. Moreover, one neurone alone is also non-invertible; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/pic2.png' | relative_url }})

Using binary neurons is an obvious choice, since we here want to design a function that outputs a binary number, which will be our public key. One thing that is very intersting with ANN, is that we can specify very easily both the size of inputs and output, such that if fits our needs, so for the rest of the article we will choose $n=256$ bits word, both for the public and private keys. So here is the architecture of our neural network:

```
# Network    
inputSize = 256
outputSize = 256
hiddenSize = inputSize * 2 # nb of neurons per hidden layer
nbHiddenLayer = 100
net = NeuralNetwork(inputSize, hiddenSize, outputSize, nbHiddenLayer)
```
![The architecture of an ANN, you can specify the size of the input and output layers, as well as the number of hidden layers and their respective sizes; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/ann.png' | relative_url }})

*NB: For the whole demonstration, I am going to use this [code]() uploaded on github. I'll share some elements here for easing comprehension, but I highly encourage you to check and try the code yourself.*

> For the whole article, the architecture is going to be constant, and we are going to play around with the weights of the ANN.

## Weight distribution

Generally the process of finding the weights is done with machine learning technics, but here we are doing it manually, because it so much more fun, and I promise It will brings interesting insights!

So before playing with the weights we first need a distribution, we are going to choose the normal distribution $\mathcal{N}(\mu, \sigma)$ parametrized by both its mean $\mu$ and standard deviation $\sigma$:

![]({{ '/assets/article_images/2022-09-24-whitepaper-p2/normal-weight.png' | relative_url }})
<center><i>The Gaussian weight distribution with parameter $\mu=-0.5$ and $\sigma=0.1$; Image generated by the author. Vertical axis displays the probability of obtaining W in horizontal axis.</i></center>

<br>

The choice of this distribution is not totally arbitrary, as it is a unimodal distribution, fully described by its first and second moments, higher order moments are null. This is an interesting property, as we will see later, because the output/input relationship will only be controlled by the two parameters $\mu$ and $\sigma$.

## Percolation

In the following we assume that the input of the network is the vector $X\in{0, 1}^n$ with $n$ the number of bits. The output vector $Y$ of the neural network is given by:

$$Y=F_{net}(X, W)$$

Where $F_{net}(.)$ is the neural network, receiving the input $X$ and parametrized by the parameter $W$. 

Now before we can decide what parameters are the best for the weight distribution, we are going to build our intuition on the behavior of this ANN. 