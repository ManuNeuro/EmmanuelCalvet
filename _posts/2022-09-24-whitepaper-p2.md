---
title: "White-paper: artificial neural networks and percolation (Part 2)"
layout: post
categories: Quantum, Crypto
comments: true
image: /assets/article_images/2022-09-24-whitepaper-p2/whitepaper-cover.jpg
---

In the previous [article](https://manuneuro.github.io/EmmanuelCalvet//quantum,/crypto/2022/09/01/whitepaper-p1.html) I introduced the concept of ellipitic curve, and how it allow for producing a *public ke*y form a *private key*. And I proposed to explore Artificial Neural Network (ANN) as a replacement. Now this is not necessary the most secure option, but this is more an excuse to be using ANN in a very non-mainstream way, so let's dive in!

I formalized the problem of generating a public key as finding a deterministic function $F$, such that:

$$public\_key = F(private\_key)$$

With $F$ being not invertible, meaning that $F^{-1}$ do no exist:

$$F^{-1}(public\_key) \ne private\_key$$

Unfortunately, invertibility in a neural network is not necessarily easy to obtain. This is because you can train a neural network to map $y=f(x)$, but you can also in theory train another neural net to get an approximation of the inverse $x=f'(x)$. However, rembember that in the first part, we gave ourselves the challenge of not using learning

This way we are going to dive into a deeper understanding of the properties needed to find a solution to the problem of desining $F$!

***

## Designing the neural network

We are going to use a binary activation function, so I'm going to use the image form the previous article, as it is going to fix ideas on what we are doing:

![The artificial neuron is a mathematical simplification of a biological neuron, composed of an activation function f (here, we use the step function). The neuron receives a weighted (W) sum of inputs (X), such that its output is y=f(X, W). Note that the action of the neuron is completely deterministic. Moreover, one neurone alone is also non-invertible; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/pic2.png' | relative_url }})

Using binary neurons is an obvious choice, since we here want to design a function that outputs a binary number, which will be our public key. One thing that is very intersting with ANN, is that we can specify very easily both the size of inputs and output, such that if fits our needs, so for the rest of the article we will choose $n=256$ bits word, both for the public and private keys. So here is the architecture of our neural network:

```
# Network    
inputSize = 256
outputSize = 256
hiddenSize = inputSize * 2 # nb of neurons per hidden layer
nbHiddenLayer = 100
net = NeuralNetwork(inputSize, hiddenSize, outputSize, nbHiddenLayer)
```
![The architecture of an ANN, you can specify the size of the input and output layers, as well as the number of hidden layers and their respective sizes; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/ann.png' | relative_url }})

> For the whole article, the architecture is going to be constant, and we are going to play around with the weights of the ANN.

In the following we assume that the input of the network is the vector $X\in \{0, 1\}^n$ with $n$ the number of bits. The output vector $Y$ of the neural network is given by:

$$\LARGE Y=F_{net}(X, W)$$

Where $F_{net}(.)$ is the neural network, receiving the input $X$ and parametrized by the parameter $W$. 

*NB: For the whole demonstration, I am going to use this [code]() uploaded on github. I'll share some elements here for easing comprehension, but I highly encourage you to check and try the code yourself.*

## Weight distribution

Generally the process of finding the weights is done with machine learning technics, but here we are doing it manually, because it so much more fun, and I promise It will brings interesting insights!

So before playing with the weights we first need a distribution, we are going to choose the normal distribution $W~\mathcal{N}(\mu, \sigma)$ parametrized by both its mean $\mu$ and standard deviation $\sigma$:

![]({{ '/assets/article_images/2022-09-24-whitepaper-p2/normal-weight.png' | relative_url }})
<center><i>The Gaussian weight distribution with parameter $\mu=-0.5$ and $\sigma=0.1$. The vertical axis displays the probability of obtaining the value W on the horizontal axis; Image generated by the author.</i></center>

<br>

The choice of this distribution is not totally arbitrary, as it is a unimodal distribution, fully described by its first and second moments, higher order moments are null. This is an interesting property, as we will see later, because the output/input relationship will only be controlled by the two parameters $\mu$ and $\sigma$.

## Percolation

Now before we can decide what parameters are the best for the weight distribution, we are going to build our intuition on the behavior of this ANN. The idea here is to get an understanding of the impact of the distribution parameters on the propagation of the bits layer to layer. Now as I will show in this section, I also choose the binary ANN because it possess a direct analogy with a real physical problem: [percolation](https://introcs.cs.princeton.edu/java/24percolation/).

In physics, percolation is the study of the propagation of a perturbation in a substrate. For example, you can imagine that someone wants to design a material, and evaluate water infiltration inside it. The typical percolation question could be: "How much damage before the water completely infiltrate my system?",or more generally:

> What is the point -in parameter space- where the perturbation cross through my system side to side?

Now this analogy is going to be evident by looking at the picture below: I plot all states of each layer as either black (1) or white (0) pixels. The horizontal axis displays the state of neurons at each layer, while the horizontal axis displays the index of the given layer. As such, the index 0 corresponds to the input layer, which is randomly initialized; and the index 101 represent the output of the network; in between indexes represent hidden layers. 

![Percolation transition: (left) only the input layer is activated, but no activity in the subsequent layers, due to the negative weight distribution. (center) activity propagate in the hierarchy, but reach an and before it can reach the last output layer. (right) the activity propagates throught the whole network and reaches the output layer.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/percolation.png' | relative_url }})

 
As one can see in the previous picture, the distribution for $\sigma=0.1$ only gave negative weights, thus this is not surprising at all that all layers are zeros. 