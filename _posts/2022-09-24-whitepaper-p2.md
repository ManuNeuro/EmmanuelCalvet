---
title: "White-paper: artificial neural networks and percolation (Part 2)"
layout: post
categories: Quantum, Crypto
comments: true
image: /assets/article_images/2022-09-24-whitepaper-p2/whitepaper-cover.jpg
---

In the previous [article](https://manuneuro.github.io/EmmanuelCalvet//quantum,/crypto/2022/09/01/whitepaper-p1.html) I introduced the concept of ellipitic curve, and how it allow for producing a *public ke*y form a *private key*. And I proposed to explore Artificial Neural Network (ANN) as a replacement. Now this is not necessary the most secure option, but this is more an excuse to be using ANN in a very non-mainstream way, so let's dive in!

I formalized the problem of generating a public key as finding a deterministic function $F$, such that:

$$public\_key = F(private\_key)$$

With $F$ being not invertible, meaning that $F^{-1}$ do no exist:

$$F^{-1}(public\_key) \ne private\_key$$

Unfortunately, invertibility in a neural network is not necessarily easy to obtain. This is because you can train a neural network to map $y=f(x)$, but you can also in theory train another neural net to get an approximation of the inverse $x=f'(x)$. However, rembember that in the first part, we gave ourselves the challenge of not using learning

This way we are going to dive into a deeper understanding of the properties needed to find a solution to the problem of desining $F$!

***

## Designing the neural network

We are going to use a binary activation function, so I'm going to use the image form the previous article, as it is going to fix ideas on what we are doing:

![The artificial neuron is a mathematical simplification of a biological neuron, composed of an activation function f (here, we use the step function). The neuron receives a weighted (W) sum of inputs (X), such that its output is y=f(X, W). Note that the action of the neuron is completely deterministic. Moreover, one neurone alone is also non-invertible; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/pic2.png' | relative_url }})

Using binary neurons is an obvious choice, since we here want to design a function that outputs a binary number, which will be our public key. One thing that is very intersting with ANN, is that we can specify very easily both the size of inputs and output, such that if fits our needs, so for the rest of the article we will choose $n=256$ bits word, both for the public and private keys. So here is the architecture of our neural network:

```
# Network    
inputSize = 256
outputSize = 256
hiddenSize = inputSize * 2 # nb of neurons per hidden layer
nbHiddenLayer = 100
net = NeuralNetwork(inputSize, hiddenSize, outputSize, nbHiddenLayer)
```
![The architecture of an ANN, you can specify the size of the input and output layers, as well as the number of hidden layers and their respective sizes; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/ann.png' | relative_url }})

> For the whole article, the architecture is going to be constant, and we are going to play around with the weights of the ANN.

In the following we assume that the input of the network is the vector $X\in \{0, 1\}^n$ with $n$ the number of bits. The output vector $Y$ of the neural network is given by:

$$\LARGE Y=F_{net}(X, W)$$

Where $F_{net}(.)$ is the neural network, receiving the input $X$ and parametrized by the parameter $W$. 

*NB: For the whole demonstration, I am going to use this [code]() uploaded on github. I'll share some elements here for easing comprehension, but I highly encourage you to check and try the code yourself.*

## Weight distribution

Generally the process of finding the weights is done with machine learning technics, but here we are doing it manually, because it so much more fun, and I promise It will brings interesting insights!

So before playing with the weights we first need a distribution, we are going to choose the normal distribution $W~\mathcal{N}(\mu, \sigma)$ parametrized by both its mean $\mu$ and standard deviation $\sigma$:

![]({{ '/assets/article_images/2022-09-24-whitepaper-p2/normal-weight.png' | relative_url }})
<center><i>The Gaussian weight distribution with parameter $\mu=-0.5$ and $\sigma=0.1$. The vertical axis displays the probability of obtaining the value W on the horizontal axis; Image generated by the author.</i></center>

<br>

The choice of this distribution is not totally arbitrary, as it is a unimodal distribution, fully described by its first and second moments, higher order moments are null. This is an interesting property, as we will see later, because the output/input relationship will only be controlled by the two parameters $\mu$ and $\sigma$.

## Percolation

Now before we can decide what parameters are the best for the weight distribution, we are going to build our intuition on the behavior of this ANN. The idea here is to get an understanding of the impact of the distribution parameters on the propagation of the bits layer to layer. Now as I will show in this section, I also choose the binary ANN because it possess a direct analogy with a real physical problem: [percolation](https://introcs.cs.princeton.edu/java/24percolation/).

In physics, percolation is the study of the propagation of a perturbation in a substrate. For example, you can imagine that someone wants to design a material, and evaluate water infiltration inside it. The typical percolation question could be: "How much damage before the water completely infiltrate my system?", or more generally:

> What is the point -in parameter space- where the perturbation crosses through the system side to side?

Now this analogy is going to be evident by looking at the picture below: I plot all states of each layer as either black (1) or white (0) pixels. The horizontal axis displays the state of neurons at each layer, while the horizontal axis is the corresponding index of the layer. As such, the index 0 corresponds to the input layer, which is randomly initialized; and the index 101 represent the output of the network; in between indexes represent hidden layers. 

![Percolation transition: (left) only the input layer is activated, but no activity in the subsequent layers, due to the negative weight distribution. (center) activity propagate in the hierarchy, but reach an and before it can reach the last output layer. (right) the activity propagates throught the whole network and reaches the output layer.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/percolation.png' | relative_url }})

 
As one can see in the previous picture, the distribution for $\sigma=0.1$ only gave negative weights, thus this is not surprising at all that all layers are zeros, indeed the only way for a neuron to be active, is to receive sum of imputs greater than zero. As we increase the weight variance $\sigma=1.5$, we start to get some positive weight in the distribution, as such, we come to a point where the bits can start propagating in the hierarchy of hidden layers. However, the bit propagation suddenly stop mid way, and the output still remains null. Now as we increase the variance again $\sigma=5$, the distribution have far more positive weights, and we can clearly see that far more neurons are active, and this activity propagate through the whole network up to the output. In that case, we reached what we call: the *percolation threshold*. 

In theory, the percolation threshold is defined as being the point in parameter space, where the the perturbation can propagate at infinity. Here we have $100$ layers, If we wanted to now more precisely the position of this point, we could increase number of hidden layer. For our purpose, $100$ is long enough, so we will just assume that the percolation threshold is reached, when the output is non-zero.

Now I want to stress out that we are randomly generating our weight matrix, and that when we are close to the percolation threshold, different seeds gives very different results! You can observe this fact in the picture below, with the same parameter $\mu=-0.5$ and $\sigma=1.5$ for four different seeds. 

![The effect of the seed: The same experiment with the same weight distribution parameter gives both percolated and non-percolated outcomes, when using different seeds.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/seed.png' | relative_url }})

The reason this happens is due to whart is called: the *sensitivity to initial condition*, and that is exactly what we are going to harness in the next part to design the $F$ function in replacement of the elliptic curve. For now, we are going to draw the phase diagram, that explicit the percolation transition, and gives us the best region for choosing our parameter $\sigma$. 

To illustrate this phenomenon, we can simply take the average of the output $\<Y\>$, obtained for different values of $\sigma$! If all output neurons are zero $\<Y\>=1/n\sum_i y_i$=0, it means that the system has not percolated. On the other hand, if even one neuron is non zero $\<Y\> \ne 0$, it means that we reached the percolation threshold. Since, for some parameter regions, we saw there is some variability in the outcomes, we will average over $10$ realizations of the same weight distribution; as a result the curve should be a bit smoother.

To generate the picture below, we took $1000$ points for $\sigma \in \[0.1, 10\]$. As excpected, there is a region of the parameter where the outpout is always zero. The percolation threshold is exactly when the activity starts to be non-zero, so in our case with $\mu=-0.5$, it is around $\sigma~1.4$. Above the percolation threshold, we have more and more activity in the network, and consequently, the output has more and more non-zero values, hence the average output increases. 

![Percolation transition: the vertical axis displays the mean output, averaged over 10 realization of the weight distribution, parametrized by the standard deviation in horizontal axis; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/output.png' | relative_url }})


***

Okay time to wrap it up: ANN with a step function activation are equivalent to a percolation model. In our case, we have seen that by fixing the mean weight, we could play with the standard deviation of the distribution, and that would be enough for controlling the propagation of bits through the network. We have shown that the parameter $\sigma$ was controlling a phase transition, from totally quiescent, to highly activated. However, sampling from the same distribution is quite noisy, and networks do not behave the same way depending on the seed we choose, something we mentioned is related to the sensitiviy to initial condition. In the next article we are going to explore this in much more detail, as this is exactly what we need to accomplish our objective! 

<center> Stay tuned! </center>


