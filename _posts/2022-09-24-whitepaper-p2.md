---
title: "White-paper: artificial neural networks and percolation (Part 2)"
layout: post
categories: Quantum, Crypto
comments: true
image: /assets/article_images/2022-09-24-whitepaper-p2/whitepaper-cover.jpg
---

In the previous [article](https://manuneuro.github.io/EmmanuelCalvet//quantum,/crypto/2022/09/01/whitepaper-p1.html), I introduced the concept of elliptic curve and how it can securely produce a *public key* form a *private key*. I also proposed to explore Artificial Neural Network (ANN) as a replacement of the elliptic curve. We are going to to design an alternative secure public key generator, with the help of ANN, so let's dive in!

I formalized the problem of generating a public key as finding a deterministic function $F$, such that:

$$\LARGE public\_key = F(private\_key)$$

With $F$ being not invertible, meaning that $F^{-1}$ does not exist:

$$\LARGE F^{-1}(public\_key) \ne private\_key$$

Unfortunately, invertibility in a neural network is not necessarily easy to obtain. This is because, if you can train a neural network to map $y=f(x)$, in theory, you can also train another neural net to approximate the inverse $x=f'(y)$. However, we will deal with this issue in the next article. 

Now, you probably remember that we gave ourselves the challenge of not using learning in the first part. This way, we will dive into a deeper understanding of the properties of the ANN!

***

## Designing the neural network

We are going to use a binary activation function, so let me reintroduce an image from my previous article, if you need a refresher on the definition of ANN:

![The artificial neuron is a mathematical simplification of a biological neuron, composed of an activation function f (here, we use the step function). The neuron receives a weighted (W) sum of inputs (X), such that its output is y=f(X, W). Note that the action of the neuron is completely deterministic. Moreover, one neurone alone is also non-invertible; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/pic2.png' | relative_url }})

I want to design a function $F$ that takes a binary word as input, and outputs a binary number. Using binary neurons is thus an obvious choice. All neuron states are going to be either 1 or 0, depending on the sign of the weighted sum they receive in input:

$$\LARGE y=f(p)= \left\{ \begin{matrix}
 1 \space\space if \space p\ge0\\
 0 \space\space if \space p<0\\
\end{matrix} \right.
$$

One thing that is very interesting with ANN, is that we can specify very easily both the size of inputs and output, such that it fits our needs, so for the rest of the article, we will choose $n=100$ bits word, both for the public and private keys. So here is the architecture of our neural network:

```
# Network    
inputSize = 100
outputSize = 100
hiddenSize = 100 # nb of neurons per hidden layer
nbHiddenLayer = 100
net = NeuralNetwork(inputSize, hiddenSize, outputSize, nbHiddenLayer)
```

![In the architecture of an ANN, you can specify the size of the input and output layers, as well as the number of hidden layers and their respective sizes; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/ann.png' | relative_url }})

> For the whole article, the architecture will be constant, and we will play around with the weights of the ANN.

In the following, we assume that the network's input is the vector $X\in \left\{0, 1\right\}^n$ with $n$ the number of bits. The output vector $Y$ of the neural network is given by:

$$\LARGE Y=F_{net}(X, W)$$

Where $F_{net}(.)$ is the neural network that receives the input $X$ and is parametrized by the parameter $W$. 

*NB: I uploaded the code for the demonstration on [GitHub](https://github.com/ManuNeuro/whitepaper.git). So I highly encourage you to check and try it by yourself.*

## Weight distribution

Generally, the process of finding the weights is done with machine learning technics, but here we are doing it manually because it is so much more fun, and I promise It will bring exciting insights!

So before playing with the weights, we first need a distribution; we are going to choose the normal distribution $W \sim \mathcal{N}(\mu, \sigma)$ parametrized by both it's mean $\mu$ and standard deviation $\sigma$:

![]({{ '/assets/article_images/2022-09-24-whitepaper-p2/normal-weight.png' | relative_url }})
<center><i>The Gaussian weight distribution with parameter $\mu=-0.5$ and $\sigma=0.1$. The vertical axis displays the probability of obtaining the value $W$ on the horizontal axis; Image generated by the author.</i></center>

<br>

The choice of this distribution is not totally arbitrary, as it is a unimodal distribution, fully described by its first and second moments, higher order moments are zero. As we will see later, this is an interesting property because the output/input relationship will only be controlled by the two parameters $\mu$ and $\sigma$.

## Percolation

Before we can decide what parameters are the best for the weight distribution, we will build our intuition on the behaviour of this ANN. The idea here is to get an understanding of the impact of the distribution parameters on the propagation of the bits layer to layer, from the input, to the output. Now as I will show in this section, I also chose the binary ANN because it possesses a direct analogy with a real physical problem: [percolation](https://introcs.cs.princeton.edu/java/24percolation/).

In physics, percolation is the study of the propagation of a perturbation in a substrate. For example, you can imagine someone wanting to design a physical system and evaluate water infiltration inside it. The typical percolation question could be: "How much damage before the water completely infiltrates my system?" 

![]({{ '/assets/article_images/2022-09-24-whitepaper-p2/crack.png' | relative_url }})

Stated in more general -and theory driven- terms, this is equivalent to ask:
> What is the point -in parameter space- where the perturbation traverses through the system side to side?

![Percolation can be seen as the question whethere a fluide, or a perturbation, or a even random walker, could go through a maze, side to side, without being stuck in the middle of it; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/schematic.png' | relative_url }})

Now, this analogy will be evident by looking at the picture below; I plot all states of each layer as either black (1) or white (0) pixels. The horizontal axis displays the state of neurons at each layer, while the horizontal axis is the corresponding index of the layer. As such, index 0 corresponds to the input layer, which is randomly initialized, and index 101 represents the network's output; in between, indexes represent hidden layers. 

![Percolation transition: (left) only the input layer is activated, but no activity in the subsequent layers due to the negative weight distribution. (center) activity propagates in the hierarchy but dies out before it can reach the last output layer. (right) the activity propagates throughout the whole network and reaches the output layer.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/percolation.png' | relative_url }})

 
As shown in the previous picture, the distribution for $\sigma=0.1$ only gave negative weights; thus it is not surprising that all layers are giving zero states; indeed, the only way for a neuron to be active is to receive a sum of inputs greater than zero. As we increase the weight variance $\sigma=1.5$, we start to get some positive weight in the distribution, so we reach a point where the bits can start propagating in the hierarchy of hidden layers. However, the bit propagation suddenly stops mid-way, and the output still remains zero. Now, as we increase the variance again $\sigma=5$, the distribution has far more positive weights, and we can see that neurons in all layers are now active; this activity indeed cascades through the whole network up to the output. In that case, we reached what we call: the *percolation threshold*. 

In theory, the percolation threshold is defined as the point in parameter space where the perturbation can propagate at infinity. Here we have $100$ layers, if we wanted to know more precisely the position of this point, we could increase the number of hidden layers. For our purpose, $100$ is long enough, and we assume that the percolation threshold is reached when the output is non-zero.

Now I want to stress that we are randomly generating our weight matrix and that when we reach the percolation threshold, different seeds will start giving very different results! You can observe this in the picture below, with the same parameters $\mu=-0.5$ and $\sigma=1.5$ for four other seeds. 

![The effect of the seed: The same experiment with the same weight distribution parameter gives both percolated and non-percolated outcomes when using different seeds; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/seed.png' | relative_url }})

This variability in the outcomes is explained by: the *sensitivity to initial condition*. This is precisely what we will harness in the next part to design the $F$ function to replace the elliptic curve. But for now, we will draw the phase diagram, which explains the percolation transition and will reveal the best region for our parameter $\sigma$. 

To illustrate the percolation transition, we can simply take the average of the output $\<Y\>$, obtained for different values of $\sigma$! If all output neurons are zero $\<Y\>=1/n\sum_i y_i$=0, the system has not percolated. On the other hand, if even one neuron is non-zero $\<Y\> \ne 0$, we reached the percolation threshold. Since, for some parameter regions, we saw there is some variability in the outcomes, we will average over $10$ realizations of the same weight distribution; as a result, the curve should be a bit smoother.

To generate the picture below, we took $1000$ points for $\sigma \in \[0.1, 10\]$. As expected, there is a parameter region where the output is always zero. The percolation threshold is exactly when the activity starts to be non-zero, so in our case with $\mu=-0.5$, it is around $\sigma~1.4$. Above the percolation threshold, we have more and more activity in the network, and consequently, the output has more and more non-zero values; hence the average output increases. Lastly, even if we averaged over ten trials, the result is still very noisy, a fact which we will explore in more detail in the following article.

![Percolation transition: the vertical axis displays the mean output averaged over 10 realizations of the weight distribution, and parametrized by the standard deviation shown in horizontal axis; Image generated by the author.]({{ '/assets/article_images/2022-09-24-whitepaper-p2/output.png' | relative_url }})


***

Okay time to wrap it up: the ANN with a step function activation is analogous to a percolation model. In our case, we have seen that by fixing the mean weight, we could play with the standard deviation of the distribution, which would be enough to control the propagation of bits through the network. We have shown that the parameter $\sigma$ controlled a phase transition from totally quiescent to highly activated. However, sampling from the same distribution is quite noisy, and networks do not behave the same way depending on the seed we choose, something we mentioned is related to the sensitivity to the initial condition. In the following article, we will explore this in much more detail, as this is related to chaos theory, exactly what we need to accomplish our objective! 

<center> Stay tuned to go down the rabbit hole! </center>


